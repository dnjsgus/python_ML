{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10f1b28",
   "metadata": {},
   "source": [
    "# Artificial neurons\n",
    "\n",
    "Input vector : $\\mathbf{x} = \\begin{bmatrix}\n",
    "                    x_1 \\\\\n",
    "                    x_2 \\\\\n",
    "                    \\vdots \\\\\n",
    "                    x_m \\end{bmatrix}$\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Weight vector : $\\mathbf{w} = \\begin{bmatrix}\n",
    "                                              w_1 \\\\\n",
    "                                              w_2 \\\\\n",
    "                                              \\vdots \\\\\n",
    "                                              w_m \\end{bmatrix}$\n",
    "                                              \n",
    "Net input: $z = \\mathbf{x}\\cdot\\mathbf{w} = w_1x_1 + w_2x_2 + ... + w_mx_m$\n",
    "\n",
    "Decision function of perceptron: $\\phi(z) = \\begin{cases}\n",
    "                             1 & \\text{if } z \\geq \\theta \\\\\n",
    "                             -1 & \\text{otherwise}\n",
    "                             \\end{cases}$\n",
    "                             \n",
    "For simplicity, $\\phi(z) = \\begin{cases}\n",
    "                           1 & \\text{if } z \\geq 0 \\\\\n",
    "                           -1 & \\text{otherwise}\n",
    "                           \\end{cases}$\n",
    "&nbsp; where $z = w_0x_0 + w_1x_1 + ... + w_mx_m$, for $w_0 = -\\theta, \\; x_0 = 1$. $w_0$ is called the ***bias unit***\n",
    "                    \n",
    "\n",
    "# Perceptron\n",
    "1. Initialize the weights to small random numbers\n",
    "2. For each training example, $\\mathbf{x}^{(i)}$ :\n",
    "    - Compute the output value, $\\hat{y}$.\n",
    "    - Update the weights. $w_j := w_j + \\Delta w_j$  &nbsp; where $\\Delta w_j = \\eta (y^{(i)} - \\hat y^{(i)})x_j^{(i)}$ &nbsp; $\\eta$ : learning rate, $y^{(i)}$ : true class label, $\\hat y^{(i)}$ : predicted class label.\n",
    "    - Converges only if two classes are linearly seperable, and learning rate is sufficiently small.\n",
    "    \n",
    "# Adaline (Adaptive linear neurons)\n",
    "Linear activation function: $\\phi(z) = z$ (Identity function)\n",
    "\n",
    "Cost function (Sum of squared errors SSE): &nbsp; $J(\\mathbf{w}) = \\frac{1}{2}\\Sigma_i (y^{(i)} - \\phi(z^{(i)}))^2$\n",
    "\n",
    "**Gradient descent**  :\n",
    "\n",
    "$w := w + \\Delta w$, where $\\Delta w = -\\eta \\nabla J(w)$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_j} = -\\Sigma_i(y^{(i)} - \\phi(z^{(i)}))x_j^{(i)}$\n",
    "\n",
    "Thus, $\\Delta w_j = -\\eta \\frac{\\partial J}{\\partial w_j} = \\eta \\Sigma_i(y^{(i)} - \\phi(z^{(i)}))x_j^{(i)}$\n",
    "\n",
    "\n",
    "**Feature scaling (Improve gradient descent)**\n",
    "\n",
    "\n",
    "Standardization : $x'_j = \\frac{x_j - \\mu_j}{\\sigma_j}$\n",
    "\n",
    "**SGD (Stochastic gradient descent)**\n",
    "\n",
    "$\\Delta w = \\eta(y^{(i)} - \\phi(z^{(i)}))x^{(i)}$   &nbsp;&nbsp; (Update for each training example)\n",
    "    \n",
    "    - Faster convergence than batch gradient descent\n",
    "    - Shuffle training dataset for every epoch to prevent cycles.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
